# ![Lexterminator Logo](src/ui/resources/logo-lexterminator-resized.png)

# Lexterminator

**Lexterminator** Ã© a primeira parte de um projeto desenvolvido para a disciplina **INE5421 - Linguagens Formais e Compiladores** na Universidade Federal de Santa Catarina (UFSC).  
Esta etapa consiste na implementaÃ§Ã£o de um **analisador lÃ©xico**, utilizando expressÃµes regulares como entrada e gerando um autÃ´mato finito determinÃ­stico (AFD) capaz de reconhecer e classificar tokens.

---

## ğŸ‘¥ Autores

- **Henrique M. Teodoro** (23100472)  
- **Jonatan F. Hartmann** (23104231)  
- **Rodrigo Schwartz** (23100471)

---

## ğŸ¯ Objetivo

A aplicaÃ§Ã£o recebe como entrada um **conjunto de definiÃ§Ãµes regulares**. As expressÃµes regulares sÃ£o convertidas em **autÃ´matos finitos**, que sÃ£o posteriormente **unidos e determinizados** para formar um Ãºnico AFD capaz de reconhecer os tokens definidos. Permite reconhecer e classificar palavras em um **arquivo de entrada com palavras a serem reconhecidas**.

---

## ğŸ“‚ OrganizaÃ§Ã£o do Projeto

```text
â”œâ”€â”€ main.py                      # InÃ­cio da aplicaÃ§Ã£o (abre GUI)
â”œâ”€â”€ Makefile                     # Comandos Ãºteis: run, clean
â”œâ”€â”€ requirements.txt             # DependÃªncias via pip
â””â”€â”€ src
    â”œâ”€â”€ control/                 # ConexÃ£o entre modelo e interface
    â”œâ”€â”€ data/
    â”‚   â”œâ”€â”€ input_regular_definitions/      # Exemplos de entrada de definiÃ§Ãµes regulares
    â”‚   â”œâ”€â”€ input_lexical_analyzer/         # Exemplos de entrada de palavras a serem reconhecidas e classificadas
    â”‚   â”œâ”€â”€ output_af/                      # SaÃ­da do AFD em texto
    â”‚   â”œâ”€â”€ output_automata_diagram/        # Imagem do autÃ´mato (PNG)
    â”‚   â””â”€â”€ output_lexical_analyzer/        # Lista de tokens reconhecidos
    â”œâ”€â”€ model/                   # ImplementaÃ§Ãµes do AutÃ´mato, Analisador LÃ©xico, Ãrvore, ExpressÃµes e DefiniÃ§Ãµes regulares.
    â”œâ”€â”€ ui/                      # Arquivos .ui (interface PyQt5) e recursos
    â”œâ”€â”€ utils/                   # FunÃ§Ãµes auxiliares
    â””â”€â”€ view/                    # Views PyQt5
```

---

## ğŸ§ª Entradas e SaÃ­das

### ğŸ“¥ Entrada:
1. **DefiniÃ§Ãµes regulares** (Exemplos: `src/data/input_regular_definitions/*.txt`):  
   - Define os **tokens vÃ¡lidos**.
   - Formato esperado:  
     ```txt
     letter: [A-za-z_]
     digit: [0-9]
     id: <letter> (<letter> | <digit>)*
     number: <digit>+
     ```

   - âš ï¸ **ValidaÃ§Ãµes**:
     - Operadores suportados: `|`, `*`, `+`, `?`, concatenaÃ§Ã£o implÃ­cita ou explÃ­cita.
     - `&` equivalente ao sÃ­mbolo vazio.
     - Suporte a agrupamentos `()`.
     - Suporte a espaÃ§o `\s`.
     - Grupos e sequÃªncias lÃ³gicas no formato `[A-Za-z0-9_]` (para A, B .., Z, a, b, .. z, 0, ..., 9, _)
     - Aliases de definiÃ§Ãµes anteriores no formato `<alias>`.
     - Uma definiÃ§Ã£o por linha, no formato `definiÃ§Ã£o: expressÃ£o` .
     - Nomes de tokens devem ser alfanumÃ©ricos e Ãºnicos.
     - **Caracteres de escape `\`**:
      - Qualquer sÃ­mbolo imediatamente apÃ³s o caractere `\` Ã© considerado um sÃ­mbolo literal, nÃ£o um operador.
      - Para representar como literal, os seguintes sÃ­mbolos devem ser escapados: `\\`, `\<`, `\>`, `\(`, `\)` `\[` `\]` `\*` `\|` `\.` `\?` `\+` `\&` `\-`, `\\s`

2. **Arquivo para anÃ¡lise lÃ©xica** (Exemplos: `src/data/input_lexical_analyzer/*.txt`):  
   - ContÃ©m o texto a ser reconhecido/tokenizado.
   - Considera uma palavra por linha.

---

### ğŸ“¤ SaÃ­da:
1. **AFD Unificado** (SaÃ­da padrÃ£o: `src/data/output_af/af_output.txt`):  
   - Mostra o autÃ´mato determinizado resultante da uniÃ£o dos AFNs.
   - Formato:
      ```txt
      nÃºmero de estados
      estado inicial
      estados finais separados por vÃ­rgula
      sÃ­mbolos do alfabeto separados por vÃ­rgula
      transiÃ§Ã£o na forma: <estado atual,sÃ­mbolo,estado destino>
      transiÃ§Ã£o na forma: <estado atual,sÃ­mbolo,estado destino>
      ...
      ```

2. **Diagrama do AutÃ´mato** (SaÃ­da padrÃ£o: `src/data/output_automata_diagram/automata_diagram.png`):  
   - Imagem gerada automaticamente usando `graphviz`.

3. **Tokens Reconhecidos** (SaÃ­da padrÃ£o: `src/data/output_lexical_analyzer/tokens_output.txt`):  
   - Lista os tokens encontrados no arquivo de entrada ou aponta erros de reconhecimento.
   - Formato:
      ```txt
      <lexema,token> // em caso de ser reconhecido como um token vÃ¡lido
      <lexema,erro!> // em caso de nÃ£o ser reconhecido como token vÃ¡lido
      ```

---

## ğŸ–¥ï¸ Interface GrÃ¡fica

A interface foi construÃ­da usando **PyQt5** e permite:
- Carregar arquivos de entrada (definiÃ§Ãµes regulares e palavras a serem reconhecidas).
- Visualizar a estrutura do AFD em diagrama e tabela.
- Acompanhar a anÃ¡lise lÃ©xica com destaque para os tokens reconhecidos no arquivo de entrada, em forma de tabela.

---

## â–¶ï¸ Como Executar

### ğŸ”§ PrÃ©-requisitos:

1. Python 3.10+
2. [Graphviz](https://graphviz.org/download/) instalado no sistema (para gerar o autÃ´mato)
3. Instalar dependÃªncias Python:
   ```bash
   pip install -r requirements.txt
   ```

### ğŸ’» Linux:
Utilize o `Makefile`:

```bash
make run     # Instala as dependÃªncias e executa a aplicaÃ§Ã£o
make clean   # Remove arquivos temporÃ¡rios
```

---

## ğŸ“¦ DependÃªncias

Listadas em `requirements.txt`. Principais:

- `PyQt5`
- `prettytable`
- `graphviz` (binding Python e pacote do sistema)

---
