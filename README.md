# ![Lexterminator Logo](src/ui/resources/logo-lexterminator-resized.png)

# Lexterminator

**Lexterminator** Ã© um projeto desenvolvido para a disciplina **INE5421 - Linguagens Formais e Compiladores** na Universidade Federal de Santa Catarina (UFSC).  
O projeto consiste na implementaÃ§Ã£o de um **analisador lÃ©xico** e de um **analisador sintÃ¡tico SLR**, formando as primeiras etapas de um compilador. O analisador lÃ©xico utiliza expressÃµes regulares como entrada para gerar um autÃ´mato finito determinÃ­stico (AFD) capaz de reconhecer e classificar os tokens da linguagem. JÃ¡ o analisador sintÃ¡tico recebe como entrada uma gramÃ¡tica livre de contexto compatÃ­vel com SLR(1) e constrÃ³i automaticamente a tabela de anÃ¡lise, sendo capaz de validar a estrutura sintÃ¡tica de programas escritos nessa linguagem.

---

## ğŸ‘¥ Autores

- **Henrique M. Teodoro** (23100472)  
- **Jonatan F. Hartmann** (23104231)  
- **Rodrigo Schwartz** (23100471)

---

## ğŸ¯ Objetivo

A aplicaÃ§Ã£o recebe como entrada um **conjunto de definiÃ§Ãµes regulares**, utilizadas para descrever os tokens da linguagem. Essas expressÃµes regulares sÃ£o convertidas em **autÃ´matos finitos nÃ£o determinÃ­sticos (AFNs)**, que sÃ£o entÃ£o unidos e convertidos em um Ãºnico **autÃ´mato finito determinÃ­stico (AFD)** capaz de reconhecer e classificar os tokens. O analisador lÃ©xico resultante Ã© capaz de processar um arquivo de entrada contendo uma sequÃªncia de palavras, reconhecendo e classificando cada uma conforme os padrÃµes definidos.

Em seguida, a **lista de tokens** reconhecidos pelo analisador lÃ©xico Ã© utilizada como entrada para o analisador sintÃ¡tico, que opera com base em uma **gramÃ¡tica livre de contexto compatÃ­vel com SLR(1)**. A partir dessa gramÃ¡tica, sÃ£o construÃ­das automaticamente as **tabelas de anÃ¡lise (ACTION e GOTO)**, permitindo ao analisador identificar se a sequÃªncia de tokens forma um programa vÃ¡lido segundo as regras sintÃ¡ticas da linguagem. Durante esse processo, sÃ£o identificadas estruturas como declaraÃ§Ãµes, comandos e expressÃµes, alÃ©m de erros sintÃ¡ticos, caso existam.

---

## ğŸ“‚ OrganizaÃ§Ã£o do Projeto

```text
â”œâ”€â”€ main.py                      # InÃ­cio da aplicaÃ§Ã£o (abre GUI)
â”œâ”€â”€ Makefile                     # Comandos Ãºteis: run, clean
â”œâ”€â”€ requirements.txt             # DependÃªncias via pip
â””â”€â”€ src
    â”œâ”€â”€ control/                 # ConexÃ£o entre modelo e interface
    â”œâ”€â”€ data/
    â”‚   â”œâ”€â”€ input_compile/                  # Exemplos de entrada de programas a serem compilados
    â”‚   â”œâ”€â”€ input_grammars/                 # Exemplos de gramÃ¡ticas SLR(1) para o analisador sintÃ¡tico
    â”‚   â”œâ”€â”€ input_regular_definitions/      # Exemplos de entrada de definiÃ§Ãµes regulares para o analisador lÃ©xico
    â”‚   â”œâ”€â”€ output_af/                      # SaÃ­da do AFD em texto
    â”‚   â”œâ”€â”€ output_automata_diagram/        # Imagem do autÃ´mato (PNG)
    â”‚   â”œâ”€â”€ output_canonical_items/         # Imagem do diagrama de itens canÃ´nicos construÃ­dos pelo analisador sintÃ¡tico
    â”‚   â”œâ”€â”€ output_lexical_analyzer/        # Lista de tokens reconhecidos
    â”‚   â”œâ”€â”€ output_parsing_table/           # Tabela de parsing feita sobre um programa compilado na anÃ¡lise sintÃ¡tica
    â”‚   â””â”€â”€ output_slr_table/               # Tabela SLR construÃ­do sobre a gramÃ¡tica de entrada
    â”œâ”€â”€ model/                   # ImplementaÃ§Ãµes relacionadas ao Analisador LÃ©xico e ao Analisador SintÃ¡tico
    â”œâ”€â”€ ui/                      # Arquivos .ui (interface PyQt5) e recursos
    â”œâ”€â”€ utils/                   # FunÃ§Ãµes auxiliares
    â””â”€â”€ view/                    # Views PyQt5
```

---

## ğŸ§ª Entradas e SaÃ­das - Analisador LÃ©xico

### ğŸ“¥ Entrada:
1. **DefiniÃ§Ãµes regulares** (Exemplos: `src/data/input_regular_definitions/*.txt`):  
   - Define os **tokens vÃ¡lidos**.
   - Formato esperado:  
     ```txt
     letter: [A-za-z_]
     digit: [0-9]
     id: <letter> (<letter> | <digit>)*
     number: <digit>+
     ```

   - âš ï¸ **ValidaÃ§Ãµes**:
     - Operadores suportados: `|`, `*`, `+`, `?`, concatenaÃ§Ã£o implÃ­cita ou explÃ­cita.
     - `&` equivalente ao sÃ­mbolo vazio.
     - Suporte a agrupamentos `()`.
     - Suporte a espaÃ§o `\s`.
     - Grupos e sequÃªncias lÃ³gicas no formato `[A-Za-z0-9_]` (para A, B .., Z, a, b, .. z, 0, ..., 9, _)
     - Aliases de definiÃ§Ãµes anteriores no formato `<alias>`.
     - Uma definiÃ§Ã£o por linha, no formato `definiÃ§Ã£o: expressÃ£o` .
     - Nomes de tokens devem ser alfanumÃ©ricos e Ãºnicos.
     - **Caracteres de escape `\`**:
      - Qualquer sÃ­mbolo imediatamente apÃ³s o caractere `\` Ã© considerado um sÃ­mbolo literal, nÃ£o um operador.
      - Para representar como literal, os seguintes sÃ­mbolos devem ser escapados: `\\`, `\<`, `\>`, `\(`, `\)` `\[` `\]` `\*` `\|` `\.` `\?` `\+` `\&` `\-`, `\\s`

2. **Arquivo para anÃ¡lise lÃ©xica** (Exemplos: `src/data/input_lexical_analyzer/*.txt`):  
   - ContÃ©m o texto a ser reconhecido/tokenizado.
   - Considera uma palavra por linha.

---

### ğŸ“¤ SaÃ­da:
1. **AFD Unificado** (SaÃ­da padrÃ£o: `src/data/output_af/af_output.txt`):  
   - Mostra o autÃ´mato determinizado resultante da uniÃ£o dos AFNs.
   - Formato:
      ```txt
      nÃºmero de estados
      estado inicial
      estados finais separados por vÃ­rgula
      sÃ­mbolos do alfabeto separados por vÃ­rgula
      transiÃ§Ã£o na forma: <estado atual,sÃ­mbolo,estado destino>
      transiÃ§Ã£o na forma: <estado atual,sÃ­mbolo,estado destino>
      ...
      ```

2. **Diagrama do AutÃ´mato** (SaÃ­da padrÃ£o: `src/data/output_automata_diagram/automata_diagram.png`):  
   - Imagem gerada automaticamente usando `graphviz`.

3. **Tokens Reconhecidos** (SaÃ­da padrÃ£o: `src/data/output_lexical_analyzer/tokens_output.txt`):  
   - Lista os tokens encontrados no arquivo de entrada ou aponta erros de reconhecimento.
   - Formato:
      ```txt
      <lexema,token> // em caso de ser reconhecido como um token vÃ¡lido
      <lexema,erro!> // em caso de nÃ£o ser reconhecido como token vÃ¡lido
      ```

---

## ğŸ§© Entradas e SaÃ­das - Analisador SintÃ¡tico

### ğŸ“¥ Entrada:

1. **GramÃ¡tica Livre de Contexto SLR(1)** (Exemplos: `src/data/input_grammars/*.txt`):

   * Define as regras sintÃ¡ticas da linguagem.
   * Formato esperado:

     ```txt
     <programa> ::= <declaracoes> <comandos> .
     <declaracoes> ::= <dcl_const> <dcl_var> <dcl_procs>
     <dcl_const> ::= const inteiro id = num_int ; <dcl_const>
     <dcl_const> ::= &
     ...
     ```
   * âš ï¸ **ValidaÃ§Ãµes e restriÃ§Ãµes**:

     * Apenas **uma produÃ§Ã£o por linha**, no formato: `<nÃ£o-terminal> ::= produÃ§Ã£o`.
     * `&` representa a **produÃ§Ã£o vazia** (Îµ).
     * O sÃ­mbolo inicial deve ser o **primeiro nÃ£o-terminal definido**.
     * O sÃ­mbolo `::=` separa o lado esquerdo do lado direito da produÃ§Ã£o.
     * Terminais e nÃ£o-terminais devem estar **separados por espaÃ§os**.
     * Terminais podem ser literais (ex: `+`, `:=`, `id`) ou palavras reservadas da linguagem.
     * NÃ£o-terminais devem estar entre **sinais de menor e maior** (ex: `<comando>`).
     * A gramÃ¡tica deve ser compatÃ­vel com **SLR(1)** â€” nÃ£o ambÃ­gua, sem conflitos shift-reduce ou reduce-reduce.
     * **Os terminais da gramÃ¡tica obrigatoriamente devem estar definidos como tokens nas definiÃ§Ãµes regulares, sem exceÃ§Ã£o.**

2. **Lista de Tokens Reconhecidos** (SaÃ­da do lÃ©xico / Exemplo: `src/data/output_lexical_analyzer/tokens_output.txt`):

   * ContÃ©m a sequÃªncia de `<lexema,token>` gerada pelo analisador lÃ©xico.
   * Apenas os **nomes dos tokens** sÃ£o usados pelo sintÃ¡tico (nÃ£o os lexemas).

---

### ğŸ“¤ SaÃ­da:

1. **Tabela SLR(1)** (SaÃ­da padrÃ£o: `src/data/output_slr_table/slr_table.csv`):
   * ContÃ©m as tabelas **ACTION** e **GOTO** utilizadas na anÃ¡lise SLR, no formato CSV.

2. **Items CanÃ´nicos** (SaÃ­da padrÃ£o: `src/data/output_canonical_items_diagram/canonical_items_diagram.png`):
   * ContÃ©m o diagrama dos items canÃ´nicos gerados na construÃ§Ã£o do analisador sintÃ¡tico

3. **Resultado da AnÃ¡lise SintÃ¡tica** (SaÃ­da padrÃ£o: `src/data/output_parsing_table/parsing_table.csv`):

   * ContÃ©m o registro passo a passo da anÃ¡lise sintÃ¡tica do programa (shift, reduce, etc.), no formato CSV.
   * A tabela finaliza com a aÃ§Ã£o "ERRO", ou "ACCEPT".

---

## ğŸ–¥ï¸ Interface GrÃ¡fica

A interface foi construÃ­da usando **PyQt5** e permite:
- Carregar arquivos de entrada (definiÃ§Ãµes regulares e gramÃ¡tica SLR(1)).
- Carregar arquivos para compilaÃ§Ã£o.
- Visualizar a estrutura do AFD em diagrama e tabela.
- Visualizar a tabela SLR e seus itens canÃ´nicos.
- Acompanhar a anÃ¡lise lÃ©xica com destaque para os tokens reconhecidos no arquivo de entrada, em forma de tabela.
- Acompanhar a anÃ¡lise sintÃ¡tica com destaque para os shift-reduces realizados, em forma de tabela.

---

## â–¶ï¸ Como Executar

### ğŸ”§ PrÃ©-requisitos:

1. Python 3.10+
2. [Graphviz](https://graphviz.org/download/) instalado no sistema (para gerar o autÃ´mato)
3. Instalar dependÃªncias Python:
   ```bash
   pip install -r requirements.txt
   ```

### ğŸ’» Linux:
Utilize o `Makefile`:

```bash
make run     # Instala as dependÃªncias e executa a aplicaÃ§Ã£o
make clean   # Remove arquivos temporÃ¡rios
```

---

## ğŸ“¦ DependÃªncias

Listadas em `requirements.txt`. Principais:

- `PyQt5`
- `prettytable`
- `graphviz` (binding Python e pacote do sistema)

---
